# Boas-vindas ao reposit√≥rio do Tech News

<details>
  <summary><strong>üë®‚Äçüíª O que foi desenvolvido</strong></summary><br />

  Projeto que tem como principal objetivo fazer consultas em not√≠cias sobre tecnologia(Raspagem de dados).

  As not√≠cias foram obtidas atrav√©s da raspagem do [_blog da Trybe_](https://blog.betrybe.com).

  <strong>üöµ Habilidades que foram trabalhadas e praticadas:</strong>
  <ul>
    <li>Utilizar o terminal interativo do Python</li>
    <li>Escrever seus pr√≥prios m√≥dulos e import√°-los em outros c√≥digos</li>
    <li>Aplicar t√©cnicas de raspagem de dados</li>
    <li>Extrair dados de conte√∫do HTML</li>
    <li>Armazenar os dados obtidos em um banco de dados</li>
  </ul>

</details>

<details>
  <summary><strong>Instala√ß√£o do projeto</strong></summary><br />

  1. Clone o reposit√≥rio

  - Use o comando: `git clone git@github.com:tryber/sd-016-a-tech-news.git`
  - Entre na pasta do reposit√≥rio que voc√™ acabou de clonar:
    - `cd sd-016-a-tech-news`

  2. Crie o ambiente virtual para o projeto

  - `python3 -m venv .venv && source .venv/bin/activate`
  
  3. Instale as depend√™ncias

  - `python3 -m pip install -r dev-requirements.txt`
</details>

# Requisitos

## 1 - Fun√ß√£o `fetch`
local: `tech_news/scraper.py`

Antes de fazer scrape, precisamos de uma p√°gina! Esta fun√ß√£o ser√° respons√°vel por fazer a requisi√ß√£o HTTP ao site e obter o conte√∫do HTML.
Alguns cuidados dever√£o ser tomados: como a fun√ß√£o poder√° ser utilizada v√°rias vezes em sucess√£o, a implementa√ß√£o respeita o  __Rate Limit__.

- A fun√ß√£o recebe uma URL
- A fun√ß√£o efetua uma requisi√ß√£o HTTP `get` para esta URL utilizando a fun√ß√£o `requests.get`
- A fun√ß√£o retorna o conte√∫do HTML da resposta.
- A fun√ß√£o respeita um Rate Limit de 1 requisi√ß√£o por segundo; Ou seja, caso chamada m√∫ltiplas vezes, ela deve aguardar 1 segundo entre cada requisi√ß√£o que fizer.
**Dica:** Uma forma simples de garantir que cada requisi√ß√£o seja feita com um intervalo m√≠nimo de um segundo √© utilizar `time.sleep(1)` antes de cada requisi√ß√£o. (Existem outras formas mais eficientes.)
- Caso a requisi√ß√£o seja bem sucedida com `Status Code 200: OK`, deve ser retornado em seu conte√∫do de texto;
- Caso a resposta tenha o c√≥digo de status diferente de `200`, √© retornado `None`;
- Caso a requisi√ß√£o n√£o receba resposta em at√© 3 segundos, ela √© abandonada (este caso √© conhecido como "Timeout"), com isso retornando None.

üìå √â preciso definir o _header_ `user-agent` para que a raspagem do blog da Trybe funcione corretamente. Com isso, foi preenchido com o valor `"Fake user-agent"` conforme exemplo abaixo:

```python
{ "user-agent": "Fake user-agent" }
```

## 2 - Fun√ß√£o `scrape_novidades`
local: `tech_news/scraper.py`

Agora no proximo passo precisamos de links para v√°rias p√°ginas de not√≠cias. Estes links est√£o contidos na p√°gina inicial do blog da Trybe (https://blog.betrybe.com). 

Esta fun√ß√£o far√° o scrape da p√°gina Novidades para obter as URLs das p√°ginas de not√≠cias. Com isso, foi utilizado a biblioteca Parsel, para obter os dados que queremos de cada p√°gina.

- A fun√ß√£o recebe uma string com o conte√∫do HTML da p√°gina inicial do blog.
- A fun√ß√£o faz o scrape do conte√∫do recebido para obter uma lista contendo as URLs das not√≠cias listadas.
- A fun√ß√£o retorna uma lista.
- Caso n√£o encontre nenhuma URL de not√≠cia, a fun√ß√£o retorna uma lista vazia.

## 3 - Fun√ß√£o `scrape_next_page_link`
local: `tech_news/scraper.py`

Para buscar mais not√≠cias, √© preciso fazer a pagina√ß√£o, e para isto, precisamos do link da pr√≥xima p√°gina. Esta fun√ß√£o √© respons√°vel por fazer o scrape deste link.

- A fun√ß√£o recebe como par√¢metro uma `string` contendo o conte√∫do HTML da p√°gina de novidades (https://blog.betrybe.com)
- A fun√ß√£o faz o scrape deste HTML para obter a URL da pr√≥xima p√°gina.
- A fun√ß√£o retorna a URL obtida.
- Caso n√£o encontre o link da pr√≥xima p√°gina, a fun√ß√£o retorna `None`

## 4 - Fun√ß√£o `scrape_noticia`
local: `tech_news/scraper.py`

Agora √© a hora de fazer o scrape dos dados que procuramos! 

- A fun√ß√£o recebe como par√¢metro o conte√∫do HTML da p√°gina de uma √∫nica not√≠cia
- A fun√ß√£o com o conte√∫do recebido, buscar as informa√ß√µes das not√≠cias para preencher um dicion√°rio com os seguintes atributos:
  - `url` - link para acesso da not√≠cia.
  - `title` - t√≠tulo da not√≠cia.
  - `timestamp` - data da not√≠cia, no formato `dd/mm/AAAA`.
  - `writer` - nome da pessoa autora da not√≠cia.
  - `comments_count` - n√∫mero de coment√°rios que a not√≠cia recebeu.
    - Se a informa√ß√£o n√£o for encontrada, salve este atributo como `0` (zero)
  - `summary` - o primeiro par√°grafo da not√≠cia.
  - `tags` - lista contendo tags da not√≠cia.
  - `category` - categoria da not√≠cia.

- Exemplo de um retorno da fun√ß√£o com uma not√≠cia fict√≠cia:

```json
{
  "url": "https://blog.betrybe.com/novidades/noticia-bacana",
  "title": "Not√≠cia bacana",
  "timestamp": "04/04/2021",
  "writer": "Eu",
  "comments_count": 4,
  "summary": "Algo muito bacana aconteceu",
  "tags": ["Tecnologia", "Esportes"],
  "category": "Ferramentas",
}
  ```

## 5 - Fun√ß√£o `get_tech_news` para obter as not√≠cias!
local: `tech_news/scraper.py`

Com estas ferramentas prontas, podemos agora fazer um scraper mais robusto com a pagina√ß√£o.

- A fun√ß√£o recebe como par√¢metro um n√∫mero inteiro `n` e buscar as √∫ltimas `n` not√≠cias do site.
- √â utilizado as fun√ß√µes `fetch`, `scrape_novidades`, `scrape_next_page_link` e `scrape_noticia` para buscar as not√≠cias e processar seu conte√∫do.
- As not√≠cias buscadas devem ser inseridas no MongoDB; 
- Ap√≥s inserir as not√≠cias no banco, a fun√ß√£o retorna estas mesmas not√≠cias.

Caso queira instalar e rodar o servidor MongoDB nativo na m√°quina, siga as instru√ß√µes no tutorial oficial:
Ubuntu: https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/  
MacOS:  https://docs.mongodb.com/guides/server/install/
  
Com o banco de dados rodando, o nosso m√≥dulo conseguir√° acess√°-lo sem problemas.

## 6 - Fun√ß√£o `search_by_title`
local: `tech_news/analyzer/search_engine.py`

Agora que temos o meios de popular o banco de dados com not√≠cias, Agora para o proximo passo foi feito a funcionalidade de busca.

- A fun√ß√£o recebe uma string com um t√≠tulo de not√≠cia
- A fun√ß√£o efetua a buscar das not√≠cias do banco de dados por t√≠tulo
- A fun√ß√£o retorna uma lista de tuplas com as not√≠cias encontradas nesta busca. 
Exemplo: 
```python
[
  ("T√≠tulo1_aqui", "url1_aqui"),
  ("T√≠tulo2_aqui", "url2_aqui"),
  ("T√≠tulo3_aqui", "url3_aqui"),
]
```
- Caso nenhuma not√≠cia seja encontrada, a fun√ß√£o retornar uma lista vazia.

## 7 - Fun√ß√£o `search_by_date`
local: `tech_news/analyzer/search_engine.py`

Esta fun√ß√£o ir√° buscar as not√≠cias do banco de dados por data.

- A fun√ß√£o recebe como par√¢metro uma data no formato ISO `AAAA-mm-dd`
- A fun√ß√£o busca as not√≠cias do banco de dados por data.
- A fun√ß√£o  retorna no mesmo formato do requisito anterior.
- Caso a data seja inv√°lida, ou esteja em outro formato, uma exce√ß√£o `ValueError` √© lan√ßada com a mensagem `Data inv√°lida`.
- Caso nenhuma not√≠cia seja encontrada, a fun√ß√£o retorna uma lista vazia.

## 8 - Fun√ß√£o `search_by_tag`,
local: `tech_news/analyzer/search_engine.py`

Esta fun√ß√£o ir√° buscar as not√≠cias por tag.

- A fun√ß√£o recebe como par√¢metro o nome da tag completo.
- A fun√ß√£o busca as not√≠cias do banco de dados por tag.
- A fun√ß√£o retorna no mesmo formato do requisito anterior.
- Caso nenhuma not√≠cia seja encontrada, a fun√ß√£o retorna uma lista vazia.

## 9 - Fun√ß√£o `search_by_category`
local: `tech_news/analyzer/search_engine.py`

Esta fun√ß√£o ir√° buscar as not√≠cias por categoria.

- A fun√ß√£o recebe como par√¢metro o nome da categoria completo.
- A fun√ß√£o busca as not√≠cias do banco de dados por categoria.
- A fun√ß√£o retorna no mesmo formato do requisito anterior.
- Caso nenhuma not√≠cia seja encontrada, a fun√ß√£o retorna uma lista vazia.